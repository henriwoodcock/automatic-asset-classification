\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Automatic Asset Classification}


\author{
  Henri L. S. Woodcock\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Asset Management Advisory\\
  Jacobs\\
  Burderop Park, Swindon \\
  \texttt{henri.woodcock@jacobs.com} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle

\begin{abstract}
\lipsum[1]
\end{abstract}


% keywords can be removed
\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
Asset management is becoming recognised as an increasingly more important part of business in all industry, not just engineering. However even though recognition is increasing there is still little research into utilising modern machine and deep learning methods to automate and optimise the methods in this discipline. This paper introduces a method to automate the classification of assets through the use of neural networks, doing this could allow us to automate the classification of assets through images from already standardised asset classes but also to find new classifications of assets in which would have never been thought of before. Exploring these new classifications could lead to new developments into managing assets and possibly improving performance in strategic asset management.

\section{Techniques}
\label{sec: techniques}
Most common methods for classification involve trying to separate points in a space. In this scenario in image classification you want to find a way to separate images in the image space. This is commonly done by rating images on similarity.

For example the k-nearest neighbours method works by classifying images based on the k-nearest image. Without any preprocessing this is done purely on pixel values.

If two images are visually similar (ie they have similar pixel values) they are classified as the same. One issue with this is it requires all images to have the object that is being classified in similar positions on the image and to be similar in appearance, of course this does not represent intelligence very much as humans are able to classify objects despite their appearance in the image.

Convolution neural networks are a very popular type of neural network in image recognition. These networks look into the local area surrounding images not just the entire image. This gives a greater ability to the model as it is able to recognise individual objects no matter the overall context of the model. To recognise different shaped objects which are classed as the same more training examples are required.

One benefit of doing this in an unsupervised way is that the model could relate assets based different attributes than humans would. For example humans classify flood assets purely based on what they do which could be a biased approach. Two assets may not perform similar at all however they are depreciated similarly due to the role they play. An unsupervised neural network will hopefully uncover hidden attributes which are better at relating assets. An example could be the different materials used or the location they are placed in.

\subsection{Auto-Encoder}
\label{ss:ae}
An autoencoder is a special type of neural network. It is trained to recreate itself after applying a bottleneck. The bottleneck here is a encoder, which essentially compresses the image into a smaller dimension. This encoded data is then passed into a decoder which attempts to decompress the data back into the original image. Auto-encoders can hopefully find hidden relationships in images and store this imformation into lower dimensions.

In the situation here it is not as important to recreate the image perfectly as it is for the autoencoder to recreate important aspects and features of the image such as hints at the location, material and shape of the actual assets and not just a recreation of the image. Recreating an image does not teach us much more other than that images can be decompressed.

\subsection{Multiple Algorithms}
\label{ss:ma}
It is common to use multiple algorithms. An autoencoder is a vary large network especially for images. High quality images are commonly 4k now which even on the first layer of an auto-encoder would give a huge amount of parameters. Sometimes images are first preprocessed to reduce the size or more commonly used to reduce the images and then a statistical analysis such as principal component analysis is applied.

\section{Model}

\subsection{model 1}
Image -> Encoder -> Compressed Image -> Decoder -> Image
----------------|-> K-Nearest Neighbour -> Clusters

Image -> Encoder -> Compressed Image -> Decoder -> Image
----------------|-> K-Means -> Clusters

\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit.
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrt}
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\end{thebibliography}


\end{document}
